{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.47494858]\n",
      " [ 1.29964151]\n",
      " [ 5.36956077]\n",
      " ..., \n",
      " [ 4.49046515]\n",
      " [ 5.03594853]\n",
      " [ 3.14299441]]\n",
      "4995\n"
     ]
    }
   ],
   "source": [
    "# Make Dataset\n",
    "d = 1\n",
    "L = 6\n",
    "\n",
    "n = 10000\n",
    "x_raw = np.random.random_sample((n,d)) * L\n",
    "print(x_raw)\n",
    "\n",
    "y_raw = (np.max(x_raw % 1, axis=1) < 1 / np.power(2, 1/d) ).astype(int)\n",
    "print(sum(y_raw))\n",
    "\n",
    "p_train = 0.8\n",
    "x_train = x_raw[:int(n * p_train)]\n",
    "y_train = y_raw[:int(n * p_train)]\n",
    "x_test = x_raw[int(n * p_train):]\n",
    "y_test = y_raw[int(n * p_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build models and set up loss function and optimizer\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "simple_model = nn.Sequential(\n",
    "                nn.Linear(d, 300),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Linear(300, 2),\n",
    "              )\n",
    "\n",
    "simple_model.type(dtype)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "optimizer = optim.Adam(simple_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "num_batches = int(len(x_train) / batch_size)\n",
    "\n",
    "num_rounds = 5000\n",
    "\n",
    "rand_label = [np.random.randint(num_batches) for i in range(num_rounds)]\n",
    "\n",
    "train_batches = [(i, (x_train[rand_label[i] * batch_size: (rand_label[i] + 1) * batch_size],\n",
    "                      y_train[rand_label[i] * batch_size: (rand_label[i] + 1) * batch_size]))\n",
    "                      for i in range(num_rounds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 1000, loss = 0.0739\n",
      "t = 2000, loss = 0.0582\n",
      "t = 3000, loss = 0.0597\n",
      "t = 4000, loss = 0.0524\n",
      "t = 5000, loss = 0.0488\n"
     ]
    }
   ],
   "source": [
    "print_every = 1000\n",
    "\n",
    "for t, (x, y) in train_batches:\n",
    "    x_var = Variable(torch.from_numpy(x).type(dtype))\n",
    "    y_var = Variable(torch.from_numpy(y).type(dtype).long())\n",
    "\n",
    "    # This is the forward pass: predict the scores for each class, for each x in the batch.\n",
    "    scores = simple_model(x_var)\n",
    "    \n",
    "    # Use the correct y values and the predicted y values to compute the loss.\n",
    "    loss = loss_fn(scores, y_var)\n",
    "    \n",
    "    if (t + 1) % print_every == 0:\n",
    "        print('t = %d, loss = %.4f' % (t + 1, loss.data[0]))\n",
    "\n",
    "    # Zero out all of the gradients for the variables which the optimizer will update.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # This is the backwards pass: compute the gradient of the loss with respect to each \n",
    "    # parameter of the model.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actually update the parameters of the model using the gradients computed by the backwards pass.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.977\n"
     ]
    }
   ],
   "source": [
    "x_var = Variable(torch.from_numpy(x_test).type(dtype), volatile=True)\n",
    "scores = simple_model(x_var)\n",
    "_, preds = scores.data.cpu().max(1)\n",
    "correct_rate = (preds == torch.from_numpy(y_test)).sum() / len(x_test)\n",
    "print(correct_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
